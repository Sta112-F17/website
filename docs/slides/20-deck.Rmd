---
title: "CLT based inference"
author: "Dr. Ã‡etinkaya-Rundel"
date: "November 14, 2017"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/sta112-hex.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
opts_chunk$set(fig.height = 2.65, dpi = 300) 
# ggplot2 color palette with gray
color_palette <- list(gray = "#999999", 
                      salmon = "#E69F00", 
                      lightblue = "#56B4E9", 
                      green = "#009E73", 
                      yellow = "#F0E442", 
                      darkblue = "#0072B2", 
                      red = "#D55E00", 
                      purple = "#CC79A7")
# For nonsese...
library(emo)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(openintro)
library(infer)
library(tidyverse)
library(gridExtra)
```

class: center, middle

# Inference methods based on CLT

---

## Inference methods based on CLT

If necessary conditions are met, we can also use inference methods based on the CLT:

- use the CLT to calculate the SE of the sample statistic of interest (sample mean, 
sample proportion, difference between sample means, etc.)

- calculate the **test statistic**, number of standard errors away from the null 
value the observed sample statistic is
    - T for means, along with appropriate degrees of freedom
    - Z for proportions

- use the test statistic to calculate the **p-value**, the probability of an observed 
or more extreme outcome given that the null hypothesis is true

---

## Z distribution

.small[
Also called the **standard normal distribution**: $Z \sim N(mean = 0, \sigma = 1)$

<br/>

.pull-left[
Finding probabilities under the normal curve:

```{r}
pnorm(-1.96)
pnorm(1.96, lower.tail = FALSE)
```
]
.pull-right[
Finding cutoff values under the normal curve:

```{r}
qnorm(0.025)
qnorm(0.975)
```
]
]

---

## T distribution

- Also unimodal and symmetric, and centered at 0

- Thicker tails than the normal distribution (to make up for additional variability
introduced by using $s$ instead of $\sigma$ in calculation of the SE)

- Parameter: **degrees of freedom**

    - df for single mean: $df = n - 1$

    - df for comparing two means: 

$$df \approx \frac{(s_1^2/n_1+s_2^2/n_2)^2}{(s_1^2/n_1)^2/(n_1-1)+(s_2^2/n_2)^2/(n_2-1)} \approx min(n_1 - 1, n_2 - 1)$$

---

## T vs Z distributions

```{r echo=FALSE}
x = seq(-5,5,len=1000)
d = rbind(
  data.frame(dist="t (df=1)",  x=x, d=dt(x,1)),
  data.frame(dist="t (df=3)",  x=x, d=dt(x,3)),
  data.frame(dist="t (df=5)",  x=x, d=dt(x,5)),
  data.frame(dist="t (df=10)", x=x, d=dt(x,10)),
  data.frame(dist="t (df=15)", x=x, d=dt(x,15)),
  data.frame(dist="t (df=30)", x=x, d=dt(x,30)),
  data.frame(dist="Z", x=x, d=dnorm(x))
)

ggplot(d, aes(x=x, y=d, color=dist)) + geom_line()
```

---

## T distribution (cont.)

.small[
Finding probabilities under the t curve:
```{r}
pt(-1.96, df = 9)
pt(1.96, df = 9, lower.tail = FALSE)
```

<br/>

Finding cutoff values under the normal curve:
```{r}
qt(0.025, df = 9)
qt(0.975, df = 9)
```
]

---

class: center, middle

# Examples

---

## General Social Survey

.small[
- Since 1972, the General Social Survey (GSS) has been monitoring societal change and 
studying the growing complexity of American society. 

- The GSS aims to gather data on contemporary American society in order to 
    - monitor and explain trends and constants in attitudes, behaviors, attributes;
    - examine the structure and functioning of society in general as well as the role
    played by relevant subgroups;
    - compare the US to other societies to place American society  in comparative 
    perspective and develop cross-national models of human society;
    - make high-quality data easily accessible to scholars, students, policy makers, 
    and others, with minimal cost and waiting.

- GSS questions cover a diverse range of issues including national spending priorities, 
marijuana use, crime and punishment, race relations, quality of life, confidence in 
institutions, and sexual behavior.
]

---

## Data

2010 GSS:
```{r message=FALSE}
gss = read_csv("data/gss2010.csv")
```

<br/>

- Data dictionary at https://gssdataexplorer.norc.org/variables/vfilter

---

## Relaxing after work

<div class="question">
One of the questions on the survey is "After an average work day, about how many 
hours do you have to relax or pursue activities that you enjoy?". Do these data
provide convincing evidence that Americans, on average, spend more than 3 hours
per day relaxing? Note that the variable of interest in the dataset is `hrsrelax`.
</div>

```{r fig.height=2.5, fig.width=5}
gss %>% 
  filter(!is.na(hrsrelax)) %>%
  summarise(x_bar = mean(hrsrelax), med = median(hrsrelax), 
            sd = sd(hrsrelax), n = n())
```

---

## Exploratory Data Analysis

```{r}
ggplot(data = gss, aes(x = hrsrelax)) + 
  geom_histogram(binwidth = 1)
```

---

## Hypotheses

<div class="question">
What are the hypotheses for evaluation Americans, on average, spend more than 3 hours
per day relaxing?
</div>

--

$$H_0: \mu = 3$$ 
$$H_A: \mu > 3$$

---

## Conditions

1. Independence: The GSS uses a reasonably random sample, and the sample
size of 1,154 is less than 10% of the US population, so we can assume that the
respondents in this sample are independent of each other.

--

2. Sample size / skew: The distribution of hours relaxed is right skewed, however
the sample size is large enough for the sampling distribution to be nearly
normal.

---

## Calculating the test statistic

$$\bar{x} \sim N\left(mean = \mu, SE = \frac{\sigma}{\sqrt{n}}\right)$$

--

$$\frac{\bar{x}-\mu_0}{s/\sqrt{n}} \sim t_{df=n-1}$$

--

$$T_{df} = \frac{obs - null}{SE} = \frac{\bar{x}-\mu_0}{\frac{s}{\sqrt{n}}}$$
$$df = n - 1$$

--

```{r fig.height=3, fig.width=5}
# summary stats
hrsrelax_summ <- gss %>% 
  filter(!is.na(hrsrelax)) %>%
  summarise(xbar = mean(hrsrelax), s = sd(hrsrelax), n = n())
```

---

## 

```{r}
# calculations
(se <- hrsrelax_summ$s / sqrt(hrsrelax_summ$n))
(t <- (hrsrelax_summ$xbar - 3) / se)
(df <- hrsrelax_summ$n - 1)
```

---

## p-value

p-value = P(observed or more extreme outcome | $H_0$ true)

```{r}
pt(t, df, lower.tail = FALSE)
```

---

## Conclusion

- Since the p-value is small, we reject $H_0$.

- The data provide convincing evidence that Americans, on average, spend more than
3 hours per day relaxing after work.

<div class="question">
Would you expect a 90% confidence interval for the average number of hours Americans 
spend relaxing after work to include 3 hours?
</div>

---

## Confidence interval for a mean

$$point~estimate \pm critical~value \times SE$$

```{r}
t_star <- qt(0.95, df)
pt_est <- hrsrelax_summ$xbar
round(pt_est + c(-1,1) * t_star * se, 2)
```

<div class="question">
Interpret this interval in context of the data.
</div>

---

## In R

.small[
```{r}
# HT
t.test(gss$hrsrelax, mu = 3, alternative = "greater")

# CI
t.test(gss$hrsrelax, conf.level = 0.90)$conf.int
```
]

---

class: center, middle

# Case study

---

See the course website for **Case Study 04: Inferring from the GSS**